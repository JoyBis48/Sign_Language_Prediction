{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import clear_output, FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_v0.3.json\", 'r') as json_file:\n",
    "    wlasl_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[385, 37, 885, 720]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlasl_data[0]['instances'][0]['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
    "video_dir = os.path.join(dataset_dir, 'wlasl-processed')\n",
    "backup_dir = os.path.join(dataset_dir, 'wlasl2000-resized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_keep = [\n",
    "    \"what\", \"who\", \"where\", \"when\", \"why\", \"how\",\n",
    "    \"you\", \"I\", \"we\", \"they\", \"he\", \"she\", \"it\",\n",
    "    \"your\", \"my\", \"our\", \"their\", \"his\", \"her\",\n",
    "    \"name\", \"do\", \"go\", \"come\", \"see\", \"eat\", \"drink\",\n",
    "    \"yes\", \"no\", \"please\", \"thank\", \"sorry\",\n",
    "    \"is\", \"are\", \"am\", \"be\", \"have\", \"like\",\n",
    "    \"this\", \"that\", \"here\", \"there\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 2046.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in tqdm(range(len(wlasl_data)), ncols=100):\n",
    "    gloss = wlasl_data[i]['gloss']\n",
    "    if gloss not in classes_to_keep:\n",
    "        continue  # Skip this iteration if gloss is not in the list of classes to keep\n",
    "    instances = wlasl_data[i]['instances']\n",
    "    for instance in instances:\n",
    "        video_id = instance['video_id']\n",
    "        if os.path.exists(os.path.join(video_dir, f'{video_id}.mp4')):\n",
    "            video_path = os.path.join(video_dir, f'{video_id}.mp4')\n",
    "        elif os.path.exists(os.path.join(backup_dir, f'{video_id}.mp4')):\n",
    "            video_path = os.path.join(backup_dir, f'{video_id}.mp4')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        frame_start = instance['frame_start']\n",
    "        frame_end = instance['frame_end']\n",
    "        split = instance['split']\n",
    "        data.append({\n",
    "            'gloss': gloss,\n",
    "            'video_path': video_path,\n",
    "            'frame_start': frame_start,\n",
    "            'frame_end': frame_end,\n",
    "            'split': split\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json' target='_blank'>D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json</a><br>"
      ],
      "text/plain": [
       "D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hand = list(range(21))\n",
    "\n",
    "filtered_pose = [11, 12, 13, 14, 15, 16]\n",
    "\n",
    "filtered_face = [0, 4, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58,\n",
    "                 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105,\n",
    "                 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154,\n",
    "                 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191,\n",
    "                 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291,\n",
    "                 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324,\n",
    "                 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380,\n",
    "                 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409,\n",
    "                 415, 454, 466, 468, 473]\n",
    "\n",
    "HAND_NUM = len(filtered_hand)\n",
    "POSE_NUM = len(filtered_pose)\n",
    "FACE_NUM = len(filtered_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM + FACE_NUM, 3)) # performing preallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp.solutions.hands.Hands()\n",
    "pose = mp.solutions.pose.Pose()\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "def fetch_frame_landmarks(frame):\n",
    "    \n",
    "    all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM + FACE_NUM, 3))\n",
    "    \n",
    "    def get_hands(frame):\n",
    "        results_hands = hands.process(frame)\n",
    "        if results_hands.multi_hand_landmarks:\n",
    "            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "                if results_hands.multi_handedness[i].classification[0].index == 0: # perform classification for each hand\n",
    "                    all_landmarks[:HAND_NUM, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # right\n",
    "                else:\n",
    "                    all_landmarks[HAND_NUM:HAND_NUM * 2, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # left\n",
    "\n",
    "    def get_pose(frame):\n",
    "        results_pose = pose.process(frame)\n",
    "        if results_pose.pose_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2:HAND_NUM * 2 + POSE_NUM, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])[filtered_pose]\n",
    "        \n",
    "    def get_face(frame):\n",
    "        results_face = face_mesh.process(frame)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2 + POSE_NUM:, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark])[filtered_face]\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        executor.submit(get_hands, frame)\n",
    "        executor.submit(get_pose, frame)\n",
    "        executor.submit(get_face, frame)\n",
    "\n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def fetch_video_landmarks(video_path, start_frame=1, end_frame=-1, hands=None, pose=None, face_mesh=None):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(\"Could not open video file\")\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "         # handling edge cases\n",
    "        if start_frame <= 1:\n",
    "            start_frame = 1\n",
    "\n",
    "        elif start_frame > total_frames:\n",
    "            start_frame = 1\n",
    "            end_frame = total_frames\n",
    "            \n",
    "        if end_frame < 0 or end_frame > total_frames:\n",
    "            end_frame = total_frames\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame - 1)\n",
    "        \n",
    "        num_landmarks = HAND_NUM * 2 + POSE_NUM + FACE_NUM\n",
    "        total_frame_landmarks = np.zeros((min(end_frame, total_frames) - start_frame + 1, num_landmarks, 3))\n",
    "        \n",
    "        frame_index = start_frame\n",
    "        while frame_index <= end_frame:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame.flags.writeable = False\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_landmarks = fetch_frame_landmarks(frame)\n",
    "            total_frame_landmarks[frame_index - start_frame] = frame_landmarks\n",
    "            \n",
    "            frame_index += 1\n",
    "        \n",
    "    except IOError as e:\n",
    "        print(f\"Error opening video file: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cap.release()\n",
    "        if hands: \n",
    "            hands.reset()\n",
    "        if pose:\n",
    "            pose.reset()\n",
    "        if face_mesh:\n",
    "            face_mesh.reset()\n",
    "    \n",
    "    return total_frame_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(input_path, output_path, video_landmarks, start_frame=1, end_frame=-1):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file.\")\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # handling edge cases\n",
    "    if start_frame < 1 or start_frame > total_frames:\n",
    "        start_frame = 1\n",
    "    if end_frame < 0 or end_frame > total_frames:\n",
    "        end_frame = total_frames\n",
    "    if start_frame > end_frame:\n",
    "        raise ValueError(\"start_frame must be less than or equal to end_frame.\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    if not out.isOpened():\n",
    "        raise ValueError(\"Error opening video for output.\")\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame-1)\n",
    "    frame_index = start_frame\n",
    "    while frame_index <= end_frame and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        landmark_index = frame_index - start_frame\n",
    "        if landmark_index < len(video_landmarks):\n",
    "            frame_landmarks = video_landmarks[landmark_index]\n",
    "            landmarks = [(int(x * width), int(y * height)) for x, y, _ in frame_landmarks]\n",
    "            for x, y in landmarks:\n",
    "                cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "        \n",
    "        out.write(frame)\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference purpose only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json\", 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "test = data[205]\n",
    "output_path = r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\output.mp4\"\n",
    "video_landmarks = fetch_video_landmarks(test['video_path'],test['frame_start'],test['frame_end'])\n",
    "draw_landmarks(test['video_path'], output_path, video_landmarks, test['frame_start'],test['frame_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if data[i]['gloss'] == 'how':\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_features_dir = os.path.join(os.getcwd(), 'saved_features')\n",
    "os.makedirs(saved_features_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 491/491 [26:23<00:00,  3.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# saing the features for all the videos\n",
    "for i in tqdm(range(len(data)), ncols=100):\n",
    "    npy_path = os.path.join(saved_features_dir, f'{i}.npy')\n",
    "    if os.path.exists(npy_path): continue\n",
    "    video_path = data[i]['video_path']\n",
    "    start = data[i]['frame_start']\n",
    "    end = data[i]['frame_end']\n",
    "    \n",
    "    try:\n",
    "        video_landmarks = fetch_video_landmarks(video_path, start, end)\n",
    "        np.save(npy_path, video_landmarks)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError encoding {video_path}\\n{e}\")\n",
    "        continue   \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_dict = {}\n",
    "\n",
    "for filename in os.listdir(saved_features_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        key = filename.split('.')[0]\n",
    "        landmarks = np.load(os.path.join(saved_features_dir, filename), allow_pickle=True)\n",
    "        landmarks_dict[key] = landmarks\n",
    "\n",
    "\n",
    "np.savez_compressed('features_dict.npz', **landmarks_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='features_dict.npz' target='_blank'>features_dict.npz</a><br>"
      ],
      "text/plain": [
       "d:\\NullClass_Internship\\Sign_Language_Prediction\\features_dict.npz"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(r'features_dict.npz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
