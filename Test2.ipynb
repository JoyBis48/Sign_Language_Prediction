{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import clear_output, FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_v0.3.json\", 'r') as json_file:\n",
    "    wlasl_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[385, 37, 885, 720]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlasl_data[0]['instances'][0]['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(os.getcwd(), 'dataset')\n",
    "video_dir = os.path.join(dataset_dir, 'wlasl-processed')\n",
    "backup_dir = os.path.join(dataset_dir, 'wlasl2000-resized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_keep = [\n",
    "    \"what\", \"who\", \"where\", \"when\", \"why\", \"how\",\n",
    "    \"you\", \"I\", \"we\", \"they\", \"he\", \"she\", \"it\",\n",
    "    \"your\", \"my\", \"our\", \"their\", \"his\", \"her\",\n",
    "    \"name\", \"do\", \"go\", \"come\", \"see\", \"eat\", \"drink\",\n",
    "    \"yes\", \"no\", \"please\", \"thank\", \"sorry\",\n",
    "    \"is\", \"are\", \"am\", \"be\", \"have\", \"like\",\n",
    "    \"this\", \"that\", \"here\", \"there\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labels.txt', 'w') as file:\n",
    "    # Writing each class to a new line in the file\n",
    "    for class_name in classes_to_keep:\n",
    "        file.write(f\"{class_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 11568.07it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i in tqdm(range(len(wlasl_data)), ncols=100):\n",
    "    gloss = wlasl_data[i]['gloss']\n",
    "    if gloss not in classes_to_keep:\n",
    "        continue  # Skip this iteration if gloss is not in the list of classes to keep\n",
    "    instances = wlasl_data[i]['instances']\n",
    "    for instance in instances:\n",
    "        video_id = instance['video_id']\n",
    "        if os.path.exists(os.path.join(video_dir, f'{video_id}.mp4')):\n",
    "            video_path = os.path.join(video_dir, f'{video_id}.mp4')\n",
    "        elif os.path.exists(os.path.join(backup_dir, f'{video_id}.mp4')):\n",
    "            video_path = os.path.join(backup_dir, f'{video_id}.mp4')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        frame_start = instance['frame_start']\n",
    "        frame_end = instance['frame_end']\n",
    "        split = instance['split']\n",
    "        data.append({\n",
    "            'gloss': gloss,\n",
    "            'video_path': video_path,\n",
    "            'frame_start': frame_start,\n",
    "            'frame_end': frame_end,\n",
    "            'split': split\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json' target='_blank'>D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json</a><br>"
      ],
      "text/plain": [
       "D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hand = list(range(21))\n",
    "\n",
    "filtered_pose = [11, 12, 13, 14, 15, 16]\n",
    "\n",
    "filtered_face = [0, 4, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58,\n",
    "                 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105,\n",
    "                 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154,\n",
    "                 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191,\n",
    "                 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291,\n",
    "                 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324,\n",
    "                 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380,\n",
    "                 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409,\n",
    "                 415, 454, 466, 468, 473]\n",
    "\n",
    "HAND_NUM = len(filtered_hand)\n",
    "POSE_NUM = len(filtered_pose)\n",
    "FACE_NUM = len(filtered_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM + FACE_NUM, 3)) # performing preallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp.solutions.hands.Hands()\n",
    "pose = mp.solutions.pose.Pose()\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "def fetch_frame_landmarks(frame):\n",
    "    \n",
    "    all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM + FACE_NUM, 3))\n",
    "    \n",
    "    def get_hands(frame):\n",
    "        results_hands = hands.process(frame)\n",
    "        if results_hands.multi_hand_landmarks:\n",
    "            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "                if results_hands.multi_handedness[i].classification[0].index == 0: # perform classification for each hand\n",
    "                    all_landmarks[:HAND_NUM, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # right\n",
    "                else:\n",
    "                    all_landmarks[HAND_NUM:HAND_NUM * 2, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # left\n",
    "\n",
    "    def get_pose(frame):\n",
    "        results_pose = pose.process(frame)\n",
    "        if results_pose.pose_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2:HAND_NUM * 2 + POSE_NUM, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])[filtered_pose]\n",
    "        \n",
    "    def get_face(frame):\n",
    "        results_face = face_mesh.process(frame)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2 + POSE_NUM:, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark])[filtered_face]\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        executor.submit(get_hands, frame)\n",
    "        executor.submit(get_pose, frame)\n",
    "        executor.submit(get_face, frame)\n",
    "\n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def fetch_video_landmarks(video_path, start_frame=1, end_frame=-1, hands=None, pose=None, face_mesh=None):\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(\"Could not open video file\")\n",
    "        \n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "         # handling edge cases\n",
    "        if start_frame <= 1:\n",
    "            start_frame = 1\n",
    "\n",
    "        elif start_frame > total_frames:\n",
    "            start_frame = 1\n",
    "            end_frame = total_frames\n",
    "            \n",
    "        if end_frame < 0 or end_frame > total_frames:\n",
    "            end_frame = total_frames\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame - 1)\n",
    "        \n",
    "        num_landmarks = HAND_NUM * 2 + POSE_NUM + FACE_NUM\n",
    "        total_frame_landmarks = np.zeros((min(end_frame, total_frames) - start_frame + 1, num_landmarks, 3))\n",
    "        \n",
    "        frame_index = start_frame\n",
    "        while frame_index <= end_frame:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame.flags.writeable = False\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_landmarks = fetch_frame_landmarks(frame)\n",
    "            total_frame_landmarks[frame_index - start_frame] = frame_landmarks\n",
    "            \n",
    "            frame_index += 1\n",
    "        \n",
    "    except IOError as e:\n",
    "        print(f\"Error opening video file: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cap.release()\n",
    "        if hands: \n",
    "            hands.reset()\n",
    "        if pose:\n",
    "            pose.reset()\n",
    "        if face_mesh:\n",
    "            face_mesh.reset()\n",
    "    \n",
    "    return total_frame_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(input_path, output_path, video_landmarks, start_frame=1, end_frame=-1):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file.\")\n",
    "    \n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # handling edge cases\n",
    "    if start_frame < 1 or start_frame > total_frames:\n",
    "        start_frame = 1\n",
    "    if end_frame < 0 or end_frame > total_frames:\n",
    "        end_frame = total_frames\n",
    "    if start_frame > end_frame:\n",
    "        raise ValueError(\"start_frame must be less than or equal to end_frame.\")\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    if not out.isOpened():\n",
    "        raise ValueError(\"Error opening video for output.\")\n",
    "    \n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame-1)\n",
    "    frame_index = start_frame\n",
    "    while frame_index <= end_frame and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        landmark_index = frame_index - start_frame\n",
    "        if landmark_index < len(video_landmarks):\n",
    "            frame_landmarks = video_landmarks[landmark_index]\n",
    "            landmarks = [(int(x * width), int(y * height)) for x, y, _ in frame_landmarks]\n",
    "            for x, y in landmarks:\n",
    "                cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "        \n",
    "        out.write(frame)\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference purpose only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\dataset\\WLASL_parsed_data.json\", 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "test = data[348]\n",
    "output_path = r\"D:\\NullClass_Internship\\Sign_Language_Prediction\\output.mp4\"\n",
    "video_landmarks = fetch_video_landmarks(test['video_path'],test['frame_start'],test['frame_end'])\n",
    "draw_landmarks(test['video_path'], output_path, video_landmarks, test['frame_start'],test['frame_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if data[i]['gloss'] == 'what':\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_features_dir = os.path.join(os.getcwd(), 'saved_features')\n",
    "os.makedirs(saved_features_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saing the features for all the videos\n",
    "for i in tqdm(range(len(data)), ncols=100):\n",
    "    npy_path = os.path.join(saved_features_dir, f'{i}.npy')\n",
    "    if os.path.exists(npy_path): continue\n",
    "    video_path = data[i]['video_path']\n",
    "    start = data[i]['frame_start']\n",
    "    end = data[i]['frame_end']\n",
    "    \n",
    "    try:\n",
    "        video_landmarks = fetch_video_landmarks(video_path, start, end)\n",
    "        np.save(npy_path, video_landmarks)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError encoding {video_path}\\n{e}\")\n",
    "        continue   \n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_dict = {}\n",
    "\n",
    "for filename in os.listdir(saved_features_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        key = filename.split('.')[0]\n",
    "        landmarks = np.load(os.path.join(saved_features_dir, filename), allow_pickle=True)\n",
    "        landmarks_dict[key] = landmarks\n",
    "\n",
    "\n",
    "np.savez_compressed('features_dict.npz', **landmarks_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='features_dict.npz' target='_blank'>features_dict.npz</a><br>"
      ],
      "text/plain": [
       "d:\\NullClass_Internship\\Sign_Language_Prediction\\features_dict.npz"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(r'features_dict.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split, labels=None, max_labels=None, max_samples=None, landmarks=None, keys=None):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - split (str): The data split to filter on among 'train', 'val' and 'test'.\n",
    "    - labels (list, optional): Specific labels to include.\n",
    "    - max_labels (int, optional): Maximum number of labels to include ordered by frequency.\n",
    "    - max_samples (int, optional): Maximum number of samples to include.\n",
    "    - landmarks (list, optional): Specific landmarks to include.\n",
    "    - keys (list, optional): Specific keys to include.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: Each tuple contains (landmarks data, label) for a sample.\n",
    "    \n",
    "    \"\"\"\n",
    "    if landmarks is None:\n",
    "        landmarks = list(range(landmarks_dict['0'].shape[1]))\n",
    "    if keys is None:\n",
    "        keys = list(landmarks_dict.keys())\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    label_counts = {}\n",
    "\n",
    "    for k in keys: \n",
    "        if data[int(k)]['split'] != split: # filtering the data based on the split\n",
    "            continue\n",
    "        label = data[int(k)]['gloss'] # getting the label for each sample\n",
    "        if labels and label not in labels: # checking whether the label is in the list of labels to include\n",
    "            continue\n",
    "        if max_labels is not None:\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1 # counting the number of samples for each label\n",
    "            if label_counts[label] > max_labels: \n",
    "                continue\n",
    "        X.append(landmarks_dict[k][:, landmarks, :])\n",
    "        Y.append(label)\n",
    "        if max_samples and len(X) >= max_samples:\n",
    "            break\n",
    "\n",
    "    if max_labels is not None and not labels:\n",
    "        top_labels = sorted(label_counts, key=label_counts.get, reverse=True)[:max_labels]\n",
    "        X, Y = zip(*[(x, y) for x, y in zip(X, Y) if y in top_labels]) #filtering the data based on the top labels\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = load_data('train')\n",
    "X_val, Y_val = load_data('val')\n",
    "X_test, Y_test = load_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentation import augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 337/337 [00:11<00:00, 29.05it/s]\n"
     ]
    }
   ],
   "source": [
    "X_augmented, Y_augmented = augment(X_train, Y_train, num=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing shuffling of the datasets\n",
    "train_permute = list(range(len(Y_augmented)))\n",
    "np.random.shuffle(train_permute)\n",
    "X_train_aug = [X_augmented[i] for i in train_permute]\n",
    "Y_train_aug = [Y_augmented[i] for i in train_permute]\n",
    "\n",
    "val_permute = list(range(len(Y_val)))\n",
    "np.random.shuffle(val_permute)\n",
    "X_val = [X_val[i] for i in val_permute]\n",
    "Y_val = [Y_val[i] for i in val_permute]\n",
    "\n",
    "test_permute = list(range(len(Y_test)))\n",
    "np.random.shuffle(test_permute)\n",
    "X_test = [X_test[i] for i in test_permute]\n",
    "Y_test = [Y_test[i] for i in test_permute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data_Science_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
